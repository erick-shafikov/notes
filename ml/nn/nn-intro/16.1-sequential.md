# Sequential

```python

import torch.nn as nn


class RavelTransform(nn.Module):
    def forward(self, item):
        return item.ravel()


class DigitNN(nn.Module):
    def __init__(self, input_dim, num_hidden, output_dim):
        super().__init__()
        self.layer1 = nn.Linear(input_dim, num_hidden, bias=False)
        self.layer2 = nn.Linear(num_hidden, output_dim)
        self.bm_1 = nn.BatchNorm1d(num_hidden)

    def forward(self, x):
        x = self.layer1(x)
        x = nn.functional.relu(x)
        x = self.bm_1(x)
        x = self.layer2(x)
        return x


# sequential
# автоматически срабатывает forward
# в модели нельзя обратиться к определенным слоям
modelSeq = nn.Sequential(
    nn.Linear(28 * 28, 32),
    nn.ReLU(),
    nn.Linear(32, 10)
)
# альтернативный вариант
# в ней можно обращаться к слоям
modelSeqAdd = nn.Sequential()
modelSeqAdd.add_module('layer1', nn.Linear(28 * 28, 32))
modelSeqAdd.add_module('relu_1', nn.ReLU())
modelSeqAdd.add_module('layer_2', nn.Linear(32, 10))

```

Sequential в конструкторе

```python
import torch.nn as nn


class DigitNN(nn.Module):
    def __init__(self, input_dim, num_hidden, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(28 * 28, 32),
            nn.ReLU(),
            nn.Linear(32, 10)
        )

    def forward(self, x):
        return self.net(x)
```

Sequential как отдельный модуль

```python
import torch.nn as nn

block = nn.Sequential(
    nn.Linear(28 * 28, 32),
    nn.ReLU(),
    nn.Linear(32, 10)
)
# альтернативный вариант
# в ней можно обращаться к слоям
modelSeqAdd = nn.Sequential()
modelSeqAdd.add_module('layer1', nn.Linear(28 * 28, 32))
modelSeqAdd.add_module('relu_1', nn.ReLU())
modelSeqAdd.add_module('block', block)
modelSeqAdd.add_module('layer_2', nn.Linear(32, 10))
```

вариант с именованными

```python
import torch.nn as nn


class DigitNN(nn.Module):
    def __init__(self, input_dim, output_dim, n_layers=3):
        super().__init__()
        # каждый слой можно добавить с помощью ModuleList
        self.layers = nn.ModuleList()

        for n in range(1, n_layers + 1):
            self.layers.add_module(
                f'layer_{n}', nn.Linear(
                    input_dim // n,
                    output_dim // (n + 1)
                )
            )

        self.layer_out = nn.Linear(input_dim // (n_layers + 1), output_dim)

    def forward(self, x):
        for layer in self.net:
            x = layer(x)
            x = nn.functional.tanh(x)

        return self.layer_out(x)
```

