# Алгоритм backpropagation

реализация

```python
import torch
from random import randint


def act(z):
    return torch.tanh(z)


def df(z):
    s = act(z)
    return 1 - s * s


# функция прямого прохода
def go_forward(x_inp, w1, w2):
    z1 = torch.mv(w1[:, :3], x_inp) + w1[:, 3]
    s = act(z1)

    z2 = torch.dot(w2[:2], s) + w2[2]
    y = act(z2)
    return y, z1, z2


# генерация случайных данных и весов
torch.manual_seed(1)

# генерация случайных, начальных весов
# W1 - веса для первого нейрона
W1 = torch.rand(8).view(2, 4) - 0.5
# W2 - веса для второго нейрона
W2 = torch.rand(3) - 0.5

# обучающая выборка (она же полная выборка)
x_train = torch.FloatTensor([(-1, -1, -1), (-1, -1, 1), (-1, 1, -1), (-1, 1, 1),
                             (1, -1, -1), (1, -1, 1), (1, 1, -1), (1, 1, 1)])
y_train = torch.FloatTensor([-1, 1, -1, 1, -1, 1, -1, -1])

lmd = 0.05  # шаг обучения
N = 1000  # число итераций при обучении
total = len(y_train)  # размер обучающей выборки

for _ in range(N):
    k = randint(0, total - 1)
    x = x_train[k]  # случайный выбор образа из обучающей выборки

    # y - результат выхода второго нейрона
    # z1 - входные данные на первую функцию действия, с учетом весов
    # z2 - входные данные на вторую функцию действия, с учетом весов
    # на каждом шаге НС будет пересчитываться с новыми весами
    y, z1, z2 = go_forward(x, W1, W2)  # прямой проход по НС и вычисление выходных значений нейронов

    # так как квадратичная функция потерь, то производная - разница между фактом и обучением
    e = y - y_train[k]  # производная квадратической функции потерь

    # подставляем в производную ф-цию входные данные на нейрон, умножаем на ошибку
    # градиент предпоследнего нейрона
    delta = e * df(z2)  # вычисление локального градиента.

    # подставляем в производную второго нейрона, его входные значения
    # умножаем на градиент предыдущего шага
    delta2 = W2[:2] * delta * df(z1)  # вектор из 2-х локальных градиентов скрытого слоя

    # корректировка весов второго уровня
    W2[:2] = W2[:2] - lmd * delta * z1  # корректировка весов связей последнего слоя
    W2[2] = W2[2] - lmd * delta  # корректировка bias

    # корректировка связей первого слоя
    W1[0, :3] = W1[0, :3] - lmd * delta2[0] * x
    W1[1, :3] = W1[1, :3] - lmd * delta2[1] * x

    # корректировка bias
    W1[0, 3] = W1[0, 3] - lmd * delta2[0]
    W1[1, 3] = W1[1, 3] - lmd * delta2[1]

# тестирование обученной НС
for x, d in zip(x_train, y_train):
    y, z1, out = go_forward(x, W1, W2)
    print(f"Выходное значение НС: {y} => {d}")

# результирующие весовые коэффициенты
print(W1)
print(W2)

```