# ModuleDict

```python
import torch.nn as nn


class DigitNN(nn.Module):
    def __init__(self, input_dim, output_dim, n_layers=3, act_type=None):
        super().__init__()
        self.layers = nn.ModuleList()
        self.act_type = act_type

        for n in range(1, n_layers + 1):
            self.layers.add_module(
                f'layer_{n}', nn.Linear(
                    input_dim // n,
                    output_dim // (n + 1)
                )
            )

        self.layer_out = nn.Linear(input_dim // (n_layers + 1), output_dim)
        self.act_lis = nn.ModuleDict({
            'relu': nn.ReLU(),
            'lk_relu': nn.LeakyReLU()
        })

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
            if self.act_type and self.act_type in self.act_lis:
                x = self.act_lis[self.act_type](x)

        return self.layer_out(x)

```

Dict + Seq

```python
import torch
import torch.nn as nn

# здесь объявляйте класс модели
batch_size = 100
x = torch.rand(batch_size, 64)  # тензор x в программе не менять


class Model(nn.Module):
    def __init__(self):
        super().__init__()

        self.input = nn.Linear(64, 32, bias=True)
        self.output = nn.Linear(32, 10)

        # создание полноценных модулей, к которым можно будет обратиться из метода froward
        self.blocks = nn.ModuleDict({
            'block_1': nn.Sequential(
                nn.Linear(32, 32, bias=False),
                nn.ELU(),
                nn.BatchNorm1d(32)
            ),
            'block_2': nn.Sequential(
                nn.Linear(32, 32),
                nn.ReLU(),
                nn.Dropout(0.4)
            )
        })

    def forward(self, x, type_block='block_1'):
        block = self.blocks[type_block] if type_block in self.blocks else self.blocks['block_1']

        x = self.input(x).relu()
        x = block(x)
        return self.output(x)


model = Model()
model.eval()

predict = model(x, type_block='block_2')

```
