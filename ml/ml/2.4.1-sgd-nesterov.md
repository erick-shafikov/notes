```python
import numpy as np
from numpy.polynomial import Polynomial


# исходная функция, которую нужно аппроксимировать моделью a(x)
def func(x):
    return -0.7 * x - 0.2 * x ** 2 + 0.05 * x ** 3 - 0.2 * np.cos(3 * x) + 2


# здесь объявляйте необходимые функции
def loss(x, y, w):
    return np.mean(np.square(x @ w - y))


def dqk(x, y, w, gamma, v):
    return ((w - gamma * v) @ x.T - y) @ x


coord_x = np.arange(-4.0, 6.0, 0.1)  # значения по оси абсцисс [-4; 6] с шагом 0.1
coord_y = func(coord_x)  # значения функции по оси ординат

sz = len(coord_x)  # количество значений функций (точек)
eta = np.array([0.1, 0.01, 0.001, 0.0001])  # шаг обучения для каждого параметра w0, w1, w2, w3
w = np.array([0., 0., 0., 0.])  # начальные значения параметров модели
N = 500  # число итераций алгоритма SGD
lm = 0.02  # значение параметра лямбда для вычисления скользящего экспоненциального среднего
batch_size = 20  # размер мини-батча (величина K = 20)
gamma = 0.8  # коэффициент гамма для вычисления импульсов Нестерова
v = np.zeros(len(w))  # начальное значение [0, 0, 0, 0]

np.random.seed(0)  # генерация одинаковых последовательностей псевдослучайных чисел

x_train = np.array([[1, x, x ** 2, x ** 3] for x in coord_x])
y_train = np.array(coord_y)

Qe = loss(x_train, y_train, w)  # начальное значение среднего эмпирического риска

for _ in range(N):
    k = np.random.randint(0, sz - batch_size - 1)
    rng = range(k, k + batch_size)
    x_range = x_train[rng]
    y_range = y_train[rng]
    v = gamma * v + (1 - gamma) * eta * 2 * dqk(x_range, y_range, w, gamma, v) / batch_size

    w -= v
    Qe = lm * loss(x_range, y_range, w) + (1 - lm) * Qe

Q = loss(x_train, y_train, w)

```