# ERM — Empirical Risk Minimization

- вычисление признака качества

```python
# ВХОДНЫЕ ДАННЫЕ
import numpy as np

np.random.seed(0)  # псевдослучайные числа образуют одну и ту же последовательность (при каждом запуске)
x = np.arange(-1.0, 1.0, 0.1)  # аргумент [-1; 1] с шагом 0,1

size_train = len(x)  # размер выборки
w = [0.5, -0.3]  # коэффициенты модели
model_a = lambda m_x, m_w: (m_w[1] * m_x + m_w[0])  # модель
loss = lambda ax, y: (ax - y) ** 2  # квадратическая функция потерь

y = model_a(x, w) + np.random.normal(0, 0.1, len(x))  # целевые значения

Q = loss(model_a(x, w), y).mean()
```

- нахождение весов оптимизации квадратичной модели потерь (для линейной модели)

```python
import numpy as np

np.random.seed(0)  # псевдослучайные числа образуют одну и ту же последовательность (при каждом запуске)
x = np.arange(-1.0, 1.0, 0.1)  # аргумент [-1; 1] с шагом 0,1

model_a = lambda xx, ww: (ww[0] + ww[1] * xx)  # модель
Y = -5.2 + 0.7 * x + np.random.normal(0, 0.1, len(x))  # вектор целевых значений

X = np.stack([np.ones_like(x), x], axis=1)

print(np.linalg.inv(X.T @ X) @ X.T @ Y)

```

- нахождение весов оптимизации квадратичной модели потерь (для полиномиальной модели)

```python
import numpy as np

np.random.seed(0)  # псевдослучайные числа образуют одну и ту же последовательность (при каждом запуске)
x = np.arange(-1.0, 1.0, 0.1)  # аргумент [-1; 1] с шагом 0,1

model_a = lambda xx, ww: (ww[0] + ww[1] * xx + ww[2] * xx ** 2 + ww[3] * xx ** 3)  # модель
Y = np.sin(x * 5) + 2 * x + np.random.normal(0, 0.1, len(x))  # вектор целевых значений

X = np.array([[1, xx, xx ** 2, xx ** 3] for xx in x])  # обучающая выборка для поиска коэффициентов w модели a

Xt = X.T

w = np.linalg.inv(Xt @ X) @ Xt @ Y
```